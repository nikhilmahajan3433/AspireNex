# -*- coding: utf-8 -*-
"""CREDIT CARD FRAUD DETECTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IUS_mbITYMvV2tufjCb6RYF5K0Okyxmz
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install imbalanced-learn

# Importing Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Loading The Data
data = pd.read_csv('creditcard.csv')

# Displaying Class Distribution
print("Class Distribution:")
print(data['Class'].value_counts())

# Data Preprocessing and Rescaling

# Dropping the 'Class' column to separate predictors
predictors = data.drop(columns=['Class'])

# Checking min and max values of the original predictors
min_vals = predictors.min()
max_vals = predictors.max()
print(pd.DataFrame({'min': min_vals, 'max': max_vals}))

# Rescaling predictors
scaler = StandardScaler()
predictors_rescaled = pd.DataFrame(scaler.fit_transform(predictors), columns=predictors.columns)

# Checking min and max values after rescaling
min_vals_rescaled = predictors_rescaled.min()
max_vals_rescaled = predictors_rescaled.max()
print(pd.DataFrame({'min_after_rescaling': min_vals_rescaled, 'max_after_rescaling': max_vals_rescaled}))

# Combining rescaled predictors with the target variable
data_rescaled = pd.concat([data['Class'], predictors_rescaled], axis=1)

# Setting a random seed for reproducibility
np.random.seed(23)

# Sampling 10,000 records from the dataset
sample = data_rescaled.sample(n=10000)

print("Sampled Data:")
print(sample.head())

# Data Sampling and SMOTE

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# Define SMOTE and under-sampling strategy
smote = SMOTE(sampling_strategy=0.1, random_state=23)
undersample = RandomUnderSampler(sampling_strategy=0.5, random_state=23)

# Create a combined SMOTE and under-sampling pipeline
pipeline = Pipeline(steps=[('smote', smote), ('undersample', undersample)])

# Apply SMOTE and under-sampling
X = sample.drop(columns=['Class'])
y = sample['Class']
X_resampled, y_resampled = pipeline.fit_resample(X, y)

# Check class distribution after resampling
from collections import Counter
print("Class distribution after SMOTE and under-sampling:")
print(Counter(y_resampled))

# Combine resampled data into a single DataFrame
resampled_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='Class')], axis=1)

# Display resampled data
print("Resampled Data:")
print(resampled_data.head())

# Data Visualization Before and After SMOTE

import seaborn as sns
import matplotlib.pyplot as plt

# Before SMOTE
plt.figure(figsize=(12, 6))
sns.scatterplot(x=sample['V1'], y=sample['V2'], hue=sample['Class'].astype(str), alpha=0.3, palette={'0': 'blue', '1': 'red'})
plt.title('Before SMOTE\nFor 10,000 Random Sample')
plt.xlabel('V1')
plt.ylabel('V2')
plt.legend(title='Class', loc='upper right')
plt.show()

# After SMOTE and Under-sampling
plt.figure(figsize=(12, 6))
sns.scatterplot(x=resampled_data['V1'], y=resampled_data['V2'], hue=resampled_data['Class'].astype(str), alpha=0.3, palette={'0': 'blue', '1': 'red'})
plt.title('After SMOTE & Random Majority Under-sampling\nwith 2:1 majority:minority ratio')
plt.xlabel('V1')
plt.ylabel('V2')
plt.legend(title='Class', loc='upper right')
plt.show()

# Correlation Heatmap Before SMOTE
plt.figure(figsize=(14, 10))
corr = sample.corr()
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Heatmap Before SMOTE\nFor 10,000 Random Sample')
plt.show()

# Correlation Heatmap After SMOTE
plt.figure(figsize=(14, 10))
corr = resampled_data.corr()
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Heatmap After SMOTE & Under-sampling')
plt.show()

# Distribution Plot of 'Amount' Before and After SMOTE
plt.figure(figsize=(12, 6))
sns.histplot(sample['Amount'], kde=True, color='blue', label='Before SMOTE', alpha=0.6)
sns.histplot(resampled_data['Amount'], kde=True, color='red', label='After SMOTE', alpha=0.6)
plt.title('Distribution of Transaction Amount\nBefore and After SMOTE')
plt.xlabel('Amount')
plt.legend()
plt.show()

# Model Training and Evaluation

# Convert target variable to categorical
sample['Class'] = sample['Class'].astype('category')
train_index = sample.sample(frac=0.75, random_state=23).index

# Splitting the data into training and testing sets
train = sample.loc[train_index]
test = sample.drop(train_index)

# SMOTE for 50-50 balance
smote_50 = SMOTE(sampling_strategy=1.0, random_state=23)
X_train_50, y_train_50 = smote_50.fit_resample(train.drop(columns=['Class']), train['Class'])

# SMOTE for 75-25 balance
smote_75 = SMOTE(sampling_strategy=0.75, random_state=23)
X_train_75, y_train_75 = smote_75.fit_resample(train.drop(columns=['Class']), train['Class'])

# Train a Random Forest model on the original training data
rf = RandomForestClassifier(n_estimators=500, max_features=5, random_state=23)
rf.fit(train.drop(columns=['Class']), train['Class'])

# Predict on the test set
rf_pred = rf.predict(test.drop(columns=['Class']))
print("Confusion Matrix for Original Data:")
print(confusion_matrix(test['Class'], rf_pred))
print("Classification Report for Original Data:")
print(classification_report(test['Class'], rf_pred))

# Train a Random Forest model on the 50-50 balanced data
rf_50 = RandomForestClassifier(n_estimators=500, max_features=5, random_state=23)
rf_50.fit(X_train_50, y_train_50)

# Predict on the test set
rf_pred_50 = rf_50.predict(test.drop(columns=['Class']))
print("Confusion Matrix for 50-50 Balanced Data:")
print(confusion_matrix(test['Class'], rf_pred_50))
print("Classification Report for 50-50 Balanced Data:")
print(classification_report(test['Class'], rf_pred_50))

# Train a Random Forest model on the 75-25 balanced data
rf_75 = RandomForestClassifier(n_estimators=500, max_features=5, random_state=23)
rf_75.fit(X_train_75, y_train_75)

# Predict on the test set
rf_pred_75 = rf_75.predict(test.drop(columns=['Class']))
print("Confusion Matrix for 75-25 Balanced Data:")
print(confusion_matrix(test['Class'], rf_pred_75))
print("Classification Report for 75-25 Balanced Data:")
print(classification_report(test['Class'], rf_pred_75))

# Training other models (logistic regression, Naive Bayes, etc.) and their ensemble
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import VotingClassifier

# Create individual models
models = [
    ('lr', LogisticRegression(max_iter=1000, random_state=23)),
    ('nb', GaussianNB()),
    ('lda', LinearDiscriminantAnalysis())
]

# Ensemble model
ensemble = VotingClassifier(estimators=models, voting='soft')

# Train ensemble model
ensemble.fit(X_train_50, y_train_50)

# Predict on the validation set
ensemble_pred = ensemble.predict(test.drop(columns=['Class']))

print("Confusion Matrix for Ensemble Model:")
print(confusion_matrix(test['Class'], ensemble_pred))
print("Classification Report for Ensemble Model:")
print(classification_report(test['Class'], ensemble_pred))